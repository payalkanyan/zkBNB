{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re    # for regular expressions\n",
    "import nltk  # for text manipulation\n",
    "from nltk.corpus import stopwords\n",
    "import string # for text manipulation\n",
    "import warnings\n",
    "import pandas as pd # for data manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\") #ignore warnings\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./sample_data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-12T11:57:21.402101Z"
    }
   },
   "id": "84314d8c32ae466e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./sample_data/processed_full.csv\", sep=',')\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fc23f8216c5ebdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data[:20000]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc801f9a43d5c5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"text\"]\n",
    "data.columns = DATASET_COLUMNS\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1491362b5ac5c94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nan_count = data.isna().sum().sum()\n",
    "nan_count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b9174107812eaa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data[data['text'].notnull()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357b5fed15ff4ad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_texts(texts_list):\n",
    "    # Tüm metinleri kelimelere böl ve benzersiz kelimeleri elde et\n",
    "    all_words = set()\n",
    "    for text in texts_list:\n",
    "        words = str(text).split()\n",
    "        all_words.update(words)\n",
    "\n",
    "    # Benzersiz kelimelere bir tam sayı değeri ata\n",
    "    word_to_index = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    # Metinleri tam sayı dizisine dönüştür\n",
    "    tokenized_texts = []\n",
    "    for text in texts_list:\n",
    "        words = text.split()\n",
    "        tokenized_texts.append([word_to_index[word] for word in words])\n",
    "\n",
    "    return tokenized_texts, word_to_index\n",
    "\n",
    "texts = data['text']\n",
    "labels = data['target']\n",
    "tokenized_texts, word_to_index = tokenize_texts(texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316b4e2f536c4369"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_tokenized_texts(tokenized_texts, max_length=None):\n",
    "    if not max_length:\n",
    "        max_length = max([len(text) for text in tokenized_texts])\n",
    "\n",
    "    padded_texts = []\n",
    "    for text in tokenized_texts:\n",
    "        if len(text) < max_length:\n",
    "            text += [0] * (max_length - len(text))\n",
    "        padded_texts.append(text)\n",
    "\n",
    "    return padded_texts\n",
    "\n",
    "padded_texts = pad_tokenized_texts(tokenized_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c13758bdce09a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1  # +1 ekledik çünkü 0 için padding değerini kullanıyoruz.\n",
    "max_length = len(padded_texts[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b6cfe4d7a055dae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(vocab_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # 5 sınıf olduğu için 5 nöron kullandık ve softmax aktivasyonunu kullandık.\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c4918ee1fca6d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # 5 sınıf olduğu için 5 nöron kullandık ve softmax aktivasyonunu kullandık.\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de4f46de9ead734c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_array = np.array(labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80bed6586b3ab754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(np.array(padded_texts), labels_array, epochs=5, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "599f60b30249aca1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tf2onnx\n",
    "!pip install onnx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc0c57d8f77cda4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507cd6f60abe6463"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_signature = [tf.TensorSpec([None, max_length], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(onnx_model, \"./sample_data/model.onnx\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78aa8943f49c095c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from onnx import numpy_helper\n",
    "onnx_model   = onnx.load(\"./sample_data/model.onnx\")\n",
    "INTIALIZERS  = onnx_model.graph.initializer\n",
    "onnx_weights = {}\n",
    "for initializer in INTIALIZERS:\n",
    "    W = numpy_helper.to_array(initializer)\n",
    "    onnx_weights[initializer.name] = W"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e1eccdd0f17e92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onnx_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90cfefb5d8f70c46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
